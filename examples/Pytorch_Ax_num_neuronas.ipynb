{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch_geometric\n",
        "!pip install ax_platform\n",
        "!pip install botorch"
      ],
      "metadata": {
        "id": "TkRcjuxTlOD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH31aJazlA65"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Linear, ReLU, Sequential\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, Set2Set\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "target = 0\n",
        "dim = 64\n",
        "\n",
        "\n",
        "class MyTransform:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        data.y = data.y[:, target]  # Specify target.\n",
        "        return data\n",
        "\n",
        "\n",
        "class Complete:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(osp.dirname(osp.realpath(\"__file__\")), '..', 'data', 'QM9')\n",
        "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
        "# Originalmente: Carga y preparación del dataset\n",
        "dataset = QM9(path, transform=transform).shuffle()\n",
        "\n",
        "# Modificación para usar un subconjunto más pequeño\n",
        "# Por ejemplo, usar solo los primeros 10,000 datos del dataset\n",
        "subset_size = 1000  # Define el tamaño del subconjunto\n",
        "subset_indices = torch.randperm(len(dataset))[:subset_size]  # Selecciona índices al azar\n",
        "dataset = dataset[subset_indices]\n",
        "\n",
        "# Procede como antes\n",
        "mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
        "std = dataset.data.y.std(dim=0, keepdim=True)\n",
        "dataset.data.y = (dataset.data.y - mean) / std\n",
        "mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "# Split datasets teniendo en cuenta el nuevo tamaño\n",
        "test_dataset = dataset[:int(subset_size*0.2)]\n",
        "val_dataset = dataset[int(subset_size*0.2):int(subset_size*0.4)]\n",
        "train_dataset = dataset[int(subset_size*0.4):]\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "#________________________________________________________________________________________________\n",
        "#RED NEURONAL ARQUITECTURA\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
        "\n",
        "        nn = Sequential(Linear(5, 128), ReLU(), Linear(128, dim * dim))\n",
        "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
        "        self.gru = GRU(dim, dim)\n",
        "\n",
        "        self.set2set = Set2Set(dim, processing_steps=3)\n",
        "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "        self.lin2 = torch.nn.Linear(dim, 1)\n",
        "#________________________________________________________________________________________________\n",
        "#PROPAGACIÓN DE INFORMACIÓN\n",
        "    def forward(self, data):\n",
        "        out = F.relu(self.lin0(data.x))\n",
        "        h = out.unsqueeze(0)\n",
        "\n",
        "        for i in range(3):\n",
        "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
        "            out, h = self.gru(m.unsqueeze(0), h)\n",
        "            out = out.squeeze(0)\n",
        "\n",
        "        out = self.set2set(out, data.batch)\n",
        "        out = F.relu(self.lin1(out))\n",
        "        out = self.lin2(out)\n",
        "        return out.view(-1)\n",
        "#________________________________________________________________________________________________\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                       factor=0.7, patience=5,\n",
        "                                                       min_lr=0.00001) #ajusta el LR durante el entrenamiento.\n",
        "#________________________________________________________________________________________________\n",
        "#ENTRENAMIENTO DE RED NEURONAL\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = F.mse_loss(model(data), data.y)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_loader.dataset)\n",
        "\n",
        "#_______________________________________________________________________________________________\n",
        "#TEST RED NEURONAL\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    error = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "    return error / len(loader.dataset)\n",
        "#_______________________________________________________________________________________________\n",
        "#ENTRENAMIENTO DEL MODELO EN EPOCAS\n",
        "best_val_error = None\n",
        "for epoch in range(1, 21):\n",
        "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "    loss = train(epoch)\n",
        "    val_error = test(val_loader)\n",
        "    scheduler.step(val_error)\n",
        "\n",
        "    if best_val_error is None or val_error <= best_val_error:\n",
        "        test_error = test(test_loader)\n",
        "        best_val_error = val_error\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
        "          f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Linear, ReLU, Sequential\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, Set2Set\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.acquisition import UpperConfidenceBound\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.utils import standardize\n",
        "\n",
        "target = 0\n",
        "dim = 64\n",
        "\n",
        "\n",
        "class MyTransform:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        data.y = data.y[:, target]  # Specify target.\n",
        "        return data\n",
        "\n",
        "\n",
        "class Complete:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(osp.dirname(osp.realpath(\"__file__\")), '..', 'data', 'QM9')\n",
        "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
        "# Originalmente: Carga y preparación del dataset\n",
        "dataset = QM9(path, transform=transform).shuffle()\n",
        "\n",
        "# Modificación para usar un subconjunto más pequeño\n",
        "# Por ejemplo, usar solo los primeros 10,000 datos del dataset\n",
        "subset_size = 1000  # Define el tamaño del subconjunto\n",
        "subset_indices = torch.randperm(len(dataset))[:subset_size]  # Selecciona índices al azar\n",
        "dataset = dataset[subset_indices]\n",
        "\n",
        "# Procede como antes\n",
        "mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
        "std = dataset.data.y.std(dim=0, keepdim=True)\n",
        "dataset.data.y = (dataset.data.y - mean) / std\n",
        "mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "# Split datasets teniendo en cuenta el nuevo tamaño\n",
        "test_dataset = dataset[:int(subset_size*0.2)]\n",
        "val_dataset = dataset[int(subset_size*0.2):int(subset_size*0.4)]\n",
        "train_dataset = dataset[int(subset_size*0.4):]\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "#________________________________________________________________________________________________\n",
        "#RED NEURONAL ARQUITECTURA\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_neurons = num_neurons\n",
        "\n",
        "        self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
        "\n",
        "        nn = Sequential(Linear(5, 128), ReLU(), Linear(self.num_neurons, dim * dim))\n",
        "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
        "        self.gru = GRU(dim, dim)\n",
        "\n",
        "        self.set2set = Set2Set(dim, processing_steps=3)\n",
        "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "        self.lin2 = torch.nn.Linear(dim, 1)\n",
        "\n",
        "#________________________________________________________________________________________________\n",
        "#PROPAGACIÓN DE INFORMACIÓN\n",
        "    def forward(self, data):\n",
        "        out = F.relu(self.lin0(data.x))\n",
        "        h = out.unsqueeze(0)\n",
        "\n",
        "        for i in range(3):\n",
        "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
        "            out, h = self.gru(m.unsqueeze(0), h)\n",
        "            out = out.squeeze(0)\n",
        "\n",
        "        out = self.set2set(out, data.batch)\n",
        "        out = F.relu(self.lin1(out))\n",
        "        out = self.lin2(out)\n",
        "        return out.view(-1)\n",
        "\n",
        "#________________________________________________________________________________________________\n",
        "# Define the hyperparameter optimization settings\n",
        "num_neurons_range = [64, 128, 256, 512]\n",
        "num_epochs = 20\n",
        "num_iterations = 10\n",
        "\n",
        "# Define the objective function for hyperparameter optimization\n",
        "def objective(num_neurons):\n",
        "    val_errors = []\n",
        "    for _ in range(num_iterations):\n",
        "        model = Net(num_neurons=num_neurons)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, min_lr=0.00001)\n",
        "        best_val_error = None\n",
        "        for epoch in range(1, 21):\n",
        "            model.train()\n",
        "            loss_all = 0\n",
        "            for data in train_loader:\n",
        "                data = data.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                loss = F.mse_loss(model(data), data.y)\n",
        "                loss.backward()\n",
        "                loss_all += loss.item() * data.num_graphs\n",
        "                optimizer.step()\n",
        "            loss = loss_all / len(train_loader.dataset)\n",
        "            model.eval()\n",
        "            error = 0\n",
        "            for data in val_loader:\n",
        "                data = data.to(device)\n",
        "                error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "            val_error = error / len(val_loader.dataset)\n",
        "            scheduler.step(val_error)\n",
        "            if best_val_error is None or val_error <= best_val_error:\n",
        "                test_error = 0\n",
        "                for data in test_loader:\n",
        "                    data = data.to(device)\n",
        "                    error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "                best_val_error = val_error\n",
        "        val_errors.append(best_val_error)\n",
        "    return sum(val_errors) / len(val_errors)\n",
        "\n",
        "#________________________________________________________________________________________________\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "\n",
        "dtype = torch.double\n",
        "bounds = torch.tensor([[64.0], [512.0]], device=device, dtype=dtype)\n",
        "search_space = torch.tensor([[64.0], [128.0], [256.0], [512.0]], device=device, dtype=dtype)\n",
        "val_dataset = torch.tensor([objective(lr) for lr in search_space], device=device, dtype=dtype)\n",
        "gp = SingleTaskGP(search_space, val_dataset)\n",
        "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "\n",
        "# Perform Bayesian Optimization\n",
        "best_acqf_value = torch.tensor(float(\"inf\"))\n",
        "best_candidate = None\n",
        "for _ in range(num_candidates):\n",
        "    # Optimize acquisition function\n",
        "    candidate, acqf_value = optimize_acqf(\n",
        "        acq_function=acq_function,\n",
        "        bounds=bounds,\n",
        "        q=1,\n",
        "        num_restarts=10,\n",
        "        raw_samples=512,\n",
        "    )\n",
        "    # Update current best candidate\n",
        "    if acqf_value < best_acqf_value:\n",
        "        best_acqf_value = acqf_value\n",
        "        best_candidate = candidate\n"
      ],
      "metadata": {
        "id": "OOl6qWKvnIl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8BlcKBZ7DA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}